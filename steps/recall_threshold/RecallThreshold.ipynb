{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "673c70c6",
   "metadata": {},
   "source": [
    "## STEPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4a0a77",
   "metadata": {},
   "source": [
    "* We go through the 5 steps that are required to use a trained model and optimize it for a desired level of recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0704cda3",
   "metadata": {},
   "source": [
    "## STEP 0: LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea299f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2113a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import HfArgumentParser, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0c5b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from robust_deid.ner_datasets import DatasetSplitter, DatasetCreator, SpanFixer, SpanValidation\n",
    "from robust_deid.sequence_tagging import SequenceTagger, RecallThresholder\n",
    "from robust_deid.sequence_tagging.arguments import (\n",
    "    ModelArguments,\n",
    "    DataTrainingArguments,\n",
    "    EvaluationArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b183ce",
   "metadata": {},
   "source": [
    "## STEP 1: INITIALIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ec20f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the location where we will store the validation data\n",
    "validation_file_raw = '/home/pk621/projects/data/ehr_deidentification/i2b2/validation_unfixed.jsonl'\n",
    "# Initialize the location where we will store the validation data after fixing the spans\n",
    "validation_file = '/home/pk621/projects/data/ehr_deidentification/i2b2/validation.jsonl'\n",
    "# Initialize the location where the spans for hte validation data are stored\n",
    "validation_spans_file = '/home/pk621/projects/data/ehr_deidentification/i2b2/validation_spans.jsonl'\n",
    "# Initialize the location where we will store the sentencized and tokenized validation dataset (validation_file)\n",
    "ner_validation_file = '/home/pk621/projects/data/ehr_deidentification/ner_datasets/i2b2_train/validation.jsonl'\n",
    "# Initialize the location where we will store the model logits (predictions_file)\n",
    "# Verify this file location - Ensure it's the same location that you will pass in the json file\n",
    "# to the sequence tagger model. i.e. output_predictions_file in the json file should have the same\n",
    "# value as below\n",
    "logits_file = '/home/pk621/projects/data/ehr_deidentification/model_predictions/i2b2_train/logits.jsonl'\n",
    "# Initialize the model config. This config file contains the various parameters of the model.\n",
    "model_config = './run/i2b2/logits_i2b2.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970fe743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sentencizer and tokenizer\n",
    "sentencizer = 'en_core_sci_sm'\n",
    "tokenizer = 'clinical'\n",
    "notation = 'BILOU'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e29206",
   "metadata": {},
   "source": [
    "## STEP 2: FIX SPANS\n",
    "\n",
    "* This step is optional and may not be required\n",
    "* This code may be required if you have spans that don't line up with your tokenizer (e.g dataset was annoated at a character level and yout tokenizer doesn't split at the same position). This code fixes the spans so that the code below (creating NER datasets) runs wothout error.\n",
    "* We experienced the issue above in the step where we create the NER dataset (step 5) - where we need to align the labels with the tokens based on the BILOU/BIO.. notation. Without this step, we would run into alignment issues.\n",
    "* If you face the same issue, running this step should fix it - changes the label start and end positions of the annotated spans based on your tokenizer and saves the new spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d817d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ner_types = [\"PATIENT\", \"STAFF\", \"AGE\", \"DATE\", \"PHONE\", \"ID\", \"EMAIL\", \"PATORG\", \"LOC\", \"HOSP\", \"OTHERPHI\"]\n",
    "# Sometimes there may be some label (span) overlap - the priority list assigns a priority to each label.\n",
    "# Higher preference is given to labels with higher priority when resolving label overlap\n",
    "ner_priorities = [2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1]\n",
    "## Initialize the span fixer object\n",
    "span_fixer = SpanFixer(\n",
    "    tokenizer=tokenizer,\n",
    "    sentencizer=sentencizer,\n",
    "    ner_priorities={ner_type: priority for ner_type, priority in zip(ner_types, ner_priorities)},\n",
    "    verbose=True\n",
    ")\n",
    "## Write the dataset with the fixed test spans to a file\n",
    "with open(validation_file, 'w') as file:\n",
    "    for note in span_fixer.fix(\n",
    "        input_file=validation_file_raw,\n",
    "        text_key='text',\n",
    "        spans_key='spans'\n",
    "    ):\n",
    "        file.write(json.dumps(note) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a7fff0",
   "metadata": {},
   "source": [
    "## STEP 3: NER DATASET\n",
    "* Sentencize and tokenize the raw text. We used sentences of length 128, which includes an additional 32 context tokens on either side of the sentence. These 32 tokens serve (from the previous & next sentence) serve as additional context to the current sentence.\n",
    "* We used the en_core_sci_sm sentencizer and a custom tokenizer (can be found in the preprocessing module)\n",
    "* The dataset stored in the ner_dataset_file will be used as input to the sequence tagger model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e11941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset creator object\n",
    "dataset_creator = DatasetCreator(\n",
    "    sentencizer=sentencizer,\n",
    "    tokenizer=tokenizer,\n",
    "    max_tokens=128,\n",
    "    max_prev_sentence_token=32,\n",
    "    max_next_sentence_token=32,\n",
    "    default_chunk_size=32,\n",
    "    ignore_label='NA'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8427de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function call sentencizes and tokenizes the dataset\n",
    "# It returns a generator that iterates through the sequences.\n",
    "# We write the output to the ner_dataset_file (in json format)\n",
    "# Validation split\n",
    "ner_notes_test = dataset_creator.create(\n",
    "    input_file=validation_file,\n",
    "    mode='train',\n",
    "    notation=notation,\n",
    "    token_text_key='text',\n",
    "    metadata_key='meta',\n",
    "    note_id_key='note_id',\n",
    "    label_key='label',\n",
    "    span_text_key='spans'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0524ff54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write test ner split to file\n",
    "with open(ner_validation_file, 'w') as file:\n",
    "    for ner_sentence in ner_notes_test:\n",
    "        file.write(json.dumps(ner_sentence) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b4d1c2",
   "metadata": {},
   "source": [
    "## STEP 4: SEQUENCE TAGGING\n",
    "* Train the sequence model - specify parameters to the sequence model in the config file (model_config). The model will be trained with the specified parameters. For more information of these parameters, please refer to huggingface (or use the docs provided).\n",
    "* You can manually pass in the parameters instead of using the config file. The config file option is recommended. In our example we are passing the parameters through a config file. If you do not want to use the config file, skip the next code block and manually enter the values in the following code blocks. You will still need to read in the training args using huggingface and change values in the training args according to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0289808",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser((\n",
    "    ModelArguments,\n",
    "    DataTrainingArguments,\n",
    "    EvaluationArguments,\n",
    "    TrainingArguments\n",
    "))\n",
    "# If we pass only one argument to the script and it's the path to a json file,\n",
    "# let's parse it to get our arguments.\n",
    "model_args, data_args, evaluation_args, training_args = parser.parse_json_file(json_file=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c41e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sequence tagger\n",
    "sequence_tagger = SequenceTagger(\n",
    "    task_name=data_args.task_name,\n",
    "    notation=data_args.notation,\n",
    "    ner_types=data_args.ner_types,\n",
    "    model_name_or_path=model_args.model_name_or_path,\n",
    "    config_name=model_args.config_name,\n",
    "    tokenizer_name=model_args.tokenizer_name,\n",
    "    post_process=model_args.post_process,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    model_revision=model_args.model_revision,\n",
    "    use_auth_token=model_args.use_auth_token,\n",
    "    threshold=model_args.threshold,\n",
    "    do_lower_case=data_args.do_lower_case,\n",
    "    fp16=training_args.fp16,\n",
    "    seed=training_args.seed,\n",
    "    local_rank=training_args.local_rank\n",
    ")\n",
    "# Load the required functions of the sequence tagger\n",
    "sequence_tagger.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a904c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the required data and predictions of the sequence tagger\n",
    "# Can also use data_args.test_file instead of ner_dataset_file (make sure it matches ner_dataset_file)\n",
    "sequence_tagger.set_predict(\n",
    "    test_file=data_args.test_file,\n",
    "    max_test_samples=data_args.max_predict_samples,\n",
    "    preprocessing_num_workers=data_args.preprocessing_num_workers,\n",
    "    overwrite_cache=data_args.overwrite_cache\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a43a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the huggingface trainer\n",
    "sequence_tagger.setup_trainer(training_args=training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e36589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store predictions in the specified file\n",
    "predictions = sequence_tagger.predict()\n",
    "# Write predictions to a file\n",
    "with open(logits_file, 'w') as file:\n",
    "    for prediction in predictions:\n",
    "        file.write(json.dumps(prediction) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92406c8e",
   "metadata": {},
   "source": [
    "## STEP 5: RECALL THRESHOLDING\n",
    "* The objective is to modify the classification thresholds, i.e. instead of choosing the class with the highest probability as the prediction for a token (optimize F1), we modify the classification thresholds to optimize recall.\n",
    "* The code below is to get these thresholds such that we get the desired level of recall. We use a validation dataset to optimize the threshold and level of recall.\n",
    "* We get the thresholds by re-formulating the NER task as a binary classifiation task. PHI v/s non-PHI. We have two two methods to do this: MAX and SUM.\n",
    "* MAX: \n",
    "    - probability of PHI class = maximum SoftMax probability over all the PHI classes\n",
    "    - probability of non-PHI class\n",
    "* SUM: \n",
    "    - probability of PHI class = sum of SoftMax probabilities over all the PHI classes\n",
    "    - probability of non-PHI class\n",
    "* A brief explantion of how we use these thresholds is explained below\n",
    "* Feel free to test out differrent recall thresholds. The thresholds are computed against a validation dataset. You would then make use of these thresholds to run the evaluation or forward pass against a test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d543306",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_thresholder = RecallThresholder(notation=data_args.notation, ner_types=data_args.ner_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75e5cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold mode - max\n",
    "# This means that an input token is tagged with the non-PHI class only if the \n",
    "# maximum probability over all PHI classes was less than the chosen threshold.\n",
    "# We tag the token with the PHI class that has the highest probability\n",
    "precision, recall, threshold = recall_thresholder.get_precision_recall_threshold(\n",
    "    logits_file=logits_file,\n",
    "    recall_cutoff=99.87/100,\n",
    "    threshold_mode='max',\n",
    "    predictions_key='predictions',\n",
    "    labels_key='labels'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9c75ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Threshold Mode: ' + 'MAX')\n",
    "print('At threshold: ', threshold)\n",
    "print('Precision is: ', precision * 100)\n",
    "print('Recall is: ', recall * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e579d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold mode - sum\n",
    "# This means that an input token is tagged with the PHI class only if the sum of \n",
    "# probabilities over all PHI classes is greater than the chosen threshold.\n",
    "# We tag the token with the PHI class that has the highest probability\n",
    "precision, recall, threshold = recall_thresholder.get_precision_recall_threshold(\n",
    "    logits_file=logits_file,\n",
    "    recall_cutoff=99.87/100,\n",
    "    threshold_mode='sum',\n",
    "    predictions_key='predictions',\n",
    "    labels_key='labels'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bf8a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Threshold Mode: ' + 'SUM')\n",
    "print('At threshold: ', threshold)\n",
    "print('Precision is: ', precision * 100)\n",
    "print('Recall is: ', recall * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384b0d4f",
   "metadata": {},
   "source": [
    "## Using the thresholds\n",
    "* Once we have the thresholds - we can use these to evaluate against a dataset and/or run the forward pass against a dataset to aggressively remove PHI\n",
    "* We have 4 config files included in the run folder, two for the evaluation (max & sum) and two for the forward pass (max & sum).\n",
    "* You can make use of these config files and run the scripts/notebooks in the evaluation and forward pass folders.\n",
    "* Replace the config files (argmax) with the ones shown above (recall optimized) and the evaluation and forward pass will be run with the recall optimized models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
