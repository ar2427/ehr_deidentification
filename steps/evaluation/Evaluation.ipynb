{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "673c70c6",
   "metadata": {},
   "source": [
    "## STEPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4a0a77",
   "metadata": {},
   "source": [
    "* We go through the 5 steps that are required to use a trained model for evaluation against a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0704cda3",
   "metadata": {},
   "source": [
    "## STEP 0: LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea299f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2113a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import HfArgumentParser, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0c5b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from robust_deid.ner_datasets import DatasetSplitter, DatasetCreator, SpanFixer, SpanValidation\n",
    "from robust_deid.sequence_tagging import SequenceTagger\n",
    "from robust_deid.sequence_tagging.arguments import (\n",
    "    ModelArguments,\n",
    "    DataTrainingArguments,\n",
    "    EvaluationArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b183ce",
   "metadata": {},
   "source": [
    "## STEP 1: INITIALIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ec20f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the path where the dataset is located (input_file).\n",
    "# Initialize the location where we will store the test data\n",
    "test_file_raw = '/home/pk621/projects/data/ehr_deidentification/i2b2/test_unfixed.jsonl'\n",
    "# Initialize the location where we will store the test data after fixing the spans\n",
    "test_file = '/home/pk621/projects/data/ehr_deidentification/i2b2/test.jsonl'\n",
    "# Initialize the location where the spans for hte test data are stored\n",
    "test_spans_file = '/home/pk621/projects/data/ehr_deidentification/i2b2/test_spans.jsonl'\n",
    "# Initialize the location where we will store the sentencized and tokenized test dataset (test_file)\n",
    "ner_test_file = '/home/pk621/projects/data/ehr_deidentification/ner_datasets/i2b2_train/test.jsonl'\n",
    "\n",
    "# Initialize the model config. This config file contains the various parameters of the model.\n",
    "model_config = './run/i2b2/eval_i2b2.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970fe743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sentencizer and tokenizer\n",
    "sentencizer = 'en_core_sci_sm'\n",
    "tokenizer = 'clinical'\n",
    "notation = 'BILOU'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cd36f7",
   "metadata": {},
   "source": [
    "## STEP 2: TEST SPANS\n",
    "* We write out the test spans and also setup the token level test dataset below\n",
    "* We do this because we may have different tokenizers and to make a fair comparison, we compare models at the span level. To do this we need the span information at a character level (start of span & end of span in terms of character position). If we have information at a character level, it is easier to compare different tokenizers. Now we not only have token level performance, but also span level performance\n",
    "* We use this step to write out the annotated spans to do span level evaluation. \n",
    "* One of the reason we did this is because in step 3 we modify the original annotated spans so that we can create a NER dataset. To ensure that we still evaluate on the original annotated dataset we do this step. Read step 3 to understand further why we need do this to ensure a fair comparison during evaluation.\n",
    "* This step has the test data (or any dataset that you want to test on) in the original form - spans with the specified start and end position.\n",
    "* In summary, for doing span level evaluation we need a mapping between note_id and the annotated spans for that note_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a3a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We write out the test spans and also setup the token level test dataset below\n",
    "# We do this because we may have different tokenizers and to make a fair comparison, we compare models\n",
    "# at the span level. To do this we need the span information at a character level \n",
    "# (start of span & end of span in terms of character position). If we have information at a character\n",
    "# level, it is easier to compare different tokenizers. Now we not only have token level performance, \n",
    "# but also span level performance\n",
    "with open(test_spans_file, 'w') as file:\n",
    "    for span_info in SpanValidation.get_spans(\n",
    "            input_file=test_file_raw,\n",
    "            metadata_key='meta',\n",
    "            note_id_key='note_id',\n",
    "            spans_key='spans'):\n",
    "        file.write(json.dumps(span_info) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e29206",
   "metadata": {},
   "source": [
    "## STEP 3: FIX SPANS\n",
    "\n",
    "* This step is optional and may not be required\n",
    "* This code may be required if you have spans that don't line up with your tokenizer (e.g dataset was annoated at a character level and yout tokenizer doesn't split at the same position). This code fixes the spans so that the code below (creating NER datasets) runs wothout error.\n",
    "* We experienced the issue above in the step where we create the NER dataset (step 5) - where we need to align the labels with the tokens based on the BILOU/BIO.. notation. Without this step, we would run into alignment issues.\n",
    "* If you face the same issue, running this step should fix it - changes the label start and end positions of the annotated spans based on your tokenizer and saves the new spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d817d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ner_types = [\"PATIENT\", \"STAFF\", \"AGE\", \"DATE\", \"PHONE\", \"ID\", \"EMAIL\", \"PATORG\", \"LOC\", \"HOSP\", \"OTHERPHI\"]\n",
    "# Sometimes there may be some label (span) overlap - the priority list assigns a priority to each label.\n",
    "# Higher preference is given to labels with higher priority when resolving label overlap\n",
    "ner_priorities = [2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1]\n",
    "## Initialize the span fixer object\n",
    "span_fixer = SpanFixer(\n",
    "    tokenizer=tokenizer,\n",
    "    sentencizer=sentencizer,\n",
    "    ner_priorities={ner_type: priority for ner_type, priority in zip(ner_types, ner_priorities)},\n",
    "    verbose=True\n",
    ")\n",
    "## Write the dataset with the fixed test spans to a file\n",
    "with open(test_file, 'w') as file:\n",
    "    for note in span_fixer.fix(\n",
    "        input_file=test_file_raw,\n",
    "        text_key='text',\n",
    "        spans_key='spans'\n",
    "    ):\n",
    "        file.write(json.dumps(note) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a7fff0",
   "metadata": {},
   "source": [
    "## STEP 5: NER DATASET\n",
    "* Sentencize and tokenize the raw text. We used sentences of length 128, which includes an additional 32 context tokens on either side of the sentence. These 32 tokens serve (from the previous & next sentence) serve as additional context to the current sentence.\n",
    "* We used the en_core_sci_sm sentencizer and a custom tokenizer (can be found in the preprocessing module)\n",
    "* The dataset stored in the ner_dataset_file will be used as input to the sequence tagger model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e11941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset creator object\n",
    "dataset_creator = DatasetCreator(\n",
    "    sentencizer=sentencizer,\n",
    "    tokenizer=tokenizer,\n",
    "    max_tokens=128,\n",
    "    max_prev_sentence_token=32,\n",
    "    max_next_sentence_token=32,\n",
    "    default_chunk_size=32,\n",
    "    ignore_label='NA'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8427de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function call sentencizes and tokenizes the dataset\n",
    "# It returns a generator that iterates through the sequences.\n",
    "# We write the output to the ner_dataset_file (in json format)\n",
    "# Validation split\n",
    "ner_notes_test = dataset_creator.create(\n",
    "    input_file=test_file,\n",
    "    mode='train',\n",
    "    notation=notation,\n",
    "    token_text_key='text',\n",
    "    metadata_key='meta',\n",
    "    note_id_key='note_id',\n",
    "    label_key='label',\n",
    "    span_text_key='spans'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0524ff54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write test ner split to file\n",
    "with open(ner_test_file, 'w') as file:\n",
    "    for ner_sentence in ner_notes_test:\n",
    "        file.write(json.dumps(ner_sentence) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b4d1c2",
   "metadata": {},
   "source": [
    "## STEP 6: SEQUENCE TAGGING\n",
    "* Train the sequence model - specify parameters to the sequence model in the config file (model_config). The model will be trained with the specified parameters. For more information of these parameters, please refer to huggingface (or use the docs provided).\n",
    "* You can manually pass in the parameters instead of using the config file. The config file option is recommended. In our example we are passing the parameters through a config file. If you do not want to use the config file, skip the next code block and manually enter the values in the following code blocks. You will still need to read in the training args using huggingface and change values in the training args according to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0289808",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser((\n",
    "    ModelArguments,\n",
    "    DataTrainingArguments,\n",
    "    EvaluationArguments,\n",
    "    TrainingArguments\n",
    "))\n",
    "# If we pass only one argument to the script and it's the path to a json file,\n",
    "# let's parse it to get our arguments.\n",
    "model_args, data_args, evaluation_args, training_args = parser.parse_json_file(json_file=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c41e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sequence tagger\n",
    "sequence_tagger = SequenceTagger(\n",
    "    task_name=data_args.task_name,\n",
    "    notation=data_args.notation,\n",
    "    ner_types=data_args.ner_types,\n",
    "    model_name_or_path=model_args.model_name_or_path,\n",
    "    config_name=model_args.config_name,\n",
    "    tokenizer_name=model_args.tokenizer_name,\n",
    "    post_process=model_args.post_process,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    model_revision=model_args.model_revision,\n",
    "    use_auth_token=model_args.use_auth_token,\n",
    "    threshold=model_args.threshold,\n",
    "    do_lower_case=data_args.do_lower_case,\n",
    "    fp16=training_args.fp16,\n",
    "    seed=training_args.seed,\n",
    "    local_rank=training_args.local_rank\n",
    ")\n",
    "# Load the required functions of the sequence tagger\n",
    "sequence_tagger.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a904c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the required data for the evaluation of the sequence tagger\n",
    "sequence_tagger.set_eval(\n",
    "    validation_file=data_args.validation_file,\n",
    "    max_val_samples=data_args.max_eval_samples,\n",
    "    preprocessing_num_workers=data_args.preprocessing_num_workers,\n",
    "    overwrite_cache=data_args.overwrite_cache\n",
    ")\n",
    "sequence_tagger.set_eval_metrics(\n",
    "    validation_spans_file=evaluation_args.validation_spans_file,\n",
    "    model_eval_script=evaluation_args.model_eval_script,\n",
    "    ner_types_maps=evaluation_args.ner_type_maps,\n",
    "    evaluation_mode=evaluation_args.evaluation_mode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a43a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the huggingface trainer\n",
    "sequence_tagger.setup_trainer(training_args=training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f2f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = sequence_tagger.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f18b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(metrics, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e36589f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
